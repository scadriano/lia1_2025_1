{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<hr style=\"height:10px\">\n",
        "\n",
        "<div class='container2'>\n",
        "\t<div style='padding: 0 7em 2em 12em;'>\n",
        "\t<h1>LaboratÃ³rio de InovaÃ§Ã£o e AutomaÃ§Ã£o 1 (LIA 1)</h1>\n",
        "\t<div style=\"font-size:12pt;float:left;\"> 2025/1 | 46M34 | Sala 200 CAE</div><br><br>\n",
        "    <div style=\"font-size:12pt;float:left;\"><b>Projetos de InteligÃªncia Artificial - AnÃ¡lise ExploratÃ³ria de Dados </b></div>\n",
        "\t</div>\n",
        "</div>\n",
        "\n",
        "\n",
        " <hr style=\"height:5px\">\n",
        "\n",
        "    \n",
        "<h2>Projeto de Usando RoboFlow - <a href=\"https://universe.roboflow.com/uni-o612z/facial-emotion-recognition\">Facial emotion recognition </a> </h2>\n",
        "\n",
        "Notebook desenvolvido por: <a href=\" http://lattes.cnpq.br/3364667237527265\">Alexandre Moura Caldeira</a>\n",
        "\n",
        " <hr style=\"height:2px\">\n"
      ],
      "metadata": {
        "id": "H9x68C4Z2N0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O projeto consiste em tentar reconhecer a emoÃ§Ã£o por meio do imagens do rosto de uma pessoa\n",
        "\n",
        "* Treinamento foi feito com 30 Ã©pocas devido Ã s limitaÃ§Ãµes do Colab\n",
        "* O modelo funciona mais em videos/imagens (da minha irmÃ£) em preto e branco, devido ao fato de que existem apenas imagens em preto e branco no banco de dados\n",
        "* link dos outros videos: https://drive.google.com/drive/folders/1Fv56FXu4B3ALJ8jYrMcNsGBiXn80xNk0?usp=sharing\n",
        "* No anexo existem os videos coloridos e em preto e branco\n",
        "* NÃ­vel de confianÃ§a para que foi usado = 0.2\n",
        "* O modelo ainda estÃ¡ bem bÃ¡sico, ou seja, Ã© possivel melhorar-lo bastante\n"
      ],
      "metadata": {
        "id": "VL9SVyUh2u4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeRK0ac-H6Rh",
        "outputId": "fb4da8f1-1301-464b-cf44-27548127c1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.63-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.63-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.63\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"19rOKL31VjxICUeSlepd\")\n",
        "project = rf.workspace(\"uni-o612z\").project(\"facial-emotion-recognition\")\n",
        "version = project.version(5)\n",
        "dataset = version.download(\"yolov12\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rNNy4FsIlpz",
        "outputId": "7e4987f4-6f64-42f6-a574-d5c0da3d95f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Facial-emotion-recognition-5 to yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1218315/1218315 [00:21<00:00, 55985.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Facial-emotion-recognition-5 in yolov12:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21738/21738 [00:05<00:00, 4101.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMCknKYvKCSX",
        "outputId": "daa02e03-301e-4409-c1e8-a12753f19f54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m856.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LqBJWshKh0w",
        "outputId": "cdc64054-ff9a-450d-efb7-a528746349ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8n.pt data=/content/Facial-emotion-recognition-5/data.yaml epochs=30 imgsz=640 workers=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvhbFpJQKi95",
        "outputId": "d2add264-9880-478c-daac-3a33dceb6fbc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.25M/6.25M [00:00<00:00, 124MB/s]\n",
            "Ultralytics 8.3.128 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/Facial-emotion-recognition-5/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 19.9MB/s]\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 92.1MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 25.7Â±28.3 MB/s, size: 100.2 KB)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Facial-emotion-recognition-5/train/labels... 9485 images, 6 backgrounds, 0 corrupt: 100% 9485/9485 [00:11<00:00, 790.55it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Facial-emotion-recognition-5/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 24.5Â±23.3 MB/s, size: 121.7 KB)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Facial-emotion-recognition-5/valid/labels... 892 images, 0 backgrounds, 0 corrupt: 100% 892/892 [00:01<00:00, 563.70it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0m/content/Facial-emotion-recognition-5/valid/images/images-2020-11-06T004041-046_face_png.rf.356e9608a7ae873d0819895236165983.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Facial-emotion-recognition-5/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 30 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/30      2.09G      1.148       2.08      1.441         36        640: 100% 593/593 [03:25<00:00,  2.89it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:11<00:00,  2.46it/s]\n",
            "                   all        892        948      0.501      0.642      0.561      0.289\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/30      2.75G      1.095      1.392       1.37         45        640: 100% 593/593 [03:13<00:00,  3.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.90it/s]\n",
            "                   all        892        948      0.663      0.422      0.499      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/30      2.76G      1.072      1.236      1.347         39        640: 100% 593/593 [03:08<00:00,  3.14it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.00it/s]\n",
            "                   all        892        948      0.718        0.7      0.761      0.439\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/30      2.78G      1.015      1.131      1.311         22        640: 100% 593/593 [03:06<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.83it/s]\n",
            "                   all        892        948      0.712      0.766       0.79      0.473\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/30       2.8G     0.9555      1.027      1.274         31        640: 100% 593/593 [03:07<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.02it/s]\n",
            "                   all        892        948      0.767      0.769      0.837      0.501\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/30      2.81G     0.9347     0.9686      1.256         27        640: 100% 593/593 [03:07<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.01it/s]\n",
            "                   all        892        948      0.784      0.785      0.835      0.504\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/30      2.83G     0.9032     0.9121      1.234         38        640: 100% 593/593 [03:06<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.00it/s]\n",
            "                   all        892        948      0.828      0.798      0.849      0.508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/30      2.85G     0.8777     0.8635      1.211         55        640: 100% 593/593 [03:06<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.17it/s]\n",
            "                   all        892        948      0.841      0.815      0.886      0.558\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/30      2.87G     0.8621     0.8294        1.2         32        640: 100% 593/593 [03:06<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.93it/s]\n",
            "                   all        892        948       0.84       0.84      0.898      0.553\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/30      2.88G     0.8372     0.7939       1.19         35        640: 100% 593/593 [03:05<00:00,  3.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.04it/s]\n",
            "                   all        892        948      0.836      0.848      0.884      0.561\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/30       2.9G     0.8304     0.7701       1.18         31        640: 100% 593/593 [03:06<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.96it/s]\n",
            "                   all        892        948      0.833      0.836      0.887      0.546\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/30      2.92G      0.822     0.7514      1.173         33        640: 100% 593/593 [03:07<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:10<00:00,  2.69it/s]\n",
            "                   all        892        948      0.872      0.837      0.906      0.575\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/30      2.93G     0.8007     0.7173      1.163         27        640: 100% 593/593 [03:09<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.30it/s]\n",
            "                   all        892        948      0.868      0.866      0.923       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/30      2.95G     0.7844     0.6898      1.149         26        640: 100% 593/593 [03:07<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.16it/s]\n",
            "                   all        892        948      0.866      0.851       0.91       0.58\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/30      2.97G     0.7739     0.6913      1.143         35        640: 100% 593/593 [03:07<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.39it/s]\n",
            "                   all        892        948        0.9      0.863      0.916      0.574\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/30      2.98G     0.7663     0.6608      1.136         31        640: 100% 593/593 [03:09<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.22it/s]\n",
            "                   all        892        948      0.886      0.867      0.921      0.591\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/30         3G     0.7488     0.6457      1.128         24        640: 100% 593/593 [03:04<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.22it/s]\n",
            "                   all        892        948      0.881      0.876      0.924      0.582\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/30      3.02G     0.7414      0.617      1.121         27        640: 100% 593/593 [03:07<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.24it/s]\n",
            "                   all        892        948      0.881      0.871      0.921      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/30      3.04G     0.7362     0.6052      1.117         39        640: 100% 593/593 [03:07<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.95it/s]\n",
            "                   all        892        948      0.895      0.887      0.921      0.592\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/30      3.05G     0.7215     0.5957      1.111         39        640: 100% 593/593 [03:06<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.91it/s]\n",
            "                   all        892        948      0.893      0.872      0.925      0.588\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/30      3.07G     0.7477     0.4777      1.136         13        640: 100% 593/593 [03:01<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.23it/s]\n",
            "                   all        892        948      0.875      0.888      0.923       0.59\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/30      3.09G     0.7269     0.4454      1.116         13        640: 100% 593/593 [02:58<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.05it/s]\n",
            "                   all        892        948      0.876       0.89      0.929      0.597\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/30       3.1G     0.7111     0.4295      1.109         13        640: 100% 593/593 [02:57<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.40it/s]\n",
            "                   all        892        948      0.896      0.869      0.934      0.599\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/30      3.12G     0.6952      0.404      1.095         14        640: 100% 593/593 [02:56<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.02it/s]\n",
            "                   all        892        948      0.886      0.887      0.931      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/30      3.14G     0.6813     0.3937      1.083         13        640: 100% 593/593 [02:56<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.00it/s]\n",
            "                   all        892        948       0.92      0.881       0.93        0.6\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/30      3.15G     0.6625     0.3731      1.078         13        640: 100% 593/593 [02:57<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  3.07it/s]\n",
            "                   all        892        948      0.902      0.894      0.931      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/30      3.17G     0.6525     0.3622      1.064         13        640: 100% 593/593 [02:58<00:00,  3.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.98it/s]\n",
            "                   all        892        948      0.896      0.906      0.941      0.602\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/30      3.19G     0.6358     0.3489      1.055         15        640: 100% 593/593 [03:00<00:00,  3.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.37it/s]\n",
            "                   all        892        948      0.909      0.888      0.938      0.604\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/30      3.21G     0.6284      0.337      1.051         13        640: 100% 593/593 [03:01<00:00,  3.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.97it/s]\n",
            "                   all        892        948      0.909      0.882      0.934      0.605\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/30      3.22G      0.614     0.3261      1.039         13        640: 100% 593/593 [03:00<00:00,  3.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:08<00:00,  3.42it/s]\n",
            "                   all        892        948      0.907      0.885      0.934      0.603\n",
            "\n",
            "30 epochs completed in 1.622 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.128 ğŸš€ Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 28/28 [00:09<00:00,  2.84it/s]\n",
            "                   all        892        948        0.9       0.89      0.934      0.605\n",
            "                 angry        207        213       0.93      0.878       0.95      0.828\n",
            "                 happy        246        283      0.874      0.882      0.896      0.444\n",
            "                   sad        232        233      0.885      0.889      0.943      0.556\n",
            "             surprised        207        219      0.909      0.912      0.946      0.592\n",
            "Speed: 0.2ms preprocess, 2.0ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "ğŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    }
  ]
}